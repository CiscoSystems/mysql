#!/usr/bin/env python

import json
import sys
import subprocess
import os
import time
import commands

import utils
import ceph
import drbd

STORAGEMARKER = '/var/lib/juju/storageconfigured'
DRBD_RESOURCE = 'mysql'
DRBD_DEVICE = '/dev/drbd0'
DRBD_MOUNTPOINT = '/var/lib/mysql'

# CEPH
DATA_SRC_DST = '/var/lib/mysql'
SERVICE_NAME = utils.get_unit_name().replace('-','/').split('/')[0]
KEYRING = "/etc/ceph/ceph.client.%s.keyring" % SERVICE_NAME
KEYFILE = "/etc/ceph/ceph.client.%s.key" % SERVICE_NAME

config=json.loads(subprocess.check_output(['config-get','--format=json']))


def is_relation_made(relation=None):
    relation_data = []
    for r_id in (utils.relation_ids(relation) or []):
        for unit in (utils.relation_list(r_id) or []):
            relation_data.append(utils.relation_get_dict(relation_id=r_id,
                                 remote_unit=unit))
    if not relation_data:
        return False
    return True


def ha_relation_joined():
    # obtain the block device
    block_storage = config['block-storage']
    block_device = config['block-device']

    # Obtain the config values necessary for the cluster config. These
    # include multicast port and interface to bind to.
    corosync_bindiface = config['ha-bindiface']
    corosync_mcastport = config['ha-mcastport']

    # Starting configuring resources.
    init_services = {
            'res_mysqld':'mysql',
        }


    if block_storage == "None":
        utils.juju_log('WARNING',
                       'NO block storage configured, not passing HA relation data')
        return
    elif block_storage == "drbd":
        # Obtain resources
        resources = {
                'res_mysql_vip':'ocf:heartbeat:IPaddr2',
                'res_mysql_fs':'ocf:heartbeat:Filesystem',
                'res_mysql_drbd':'ocf:linbit:drbd',
                'res_mysqld':'upstart:mysql',
            }
        resource_params = {
                'res_mysql_vip':'params ip="%s" cidr_netmask="%s" nic="%s"' % (config['vip'],
                                config['vip_cidr'], config['vip_iface']),
                'res_mysql_fs':'params device="%s" directory="%s" fstype="ext3"' % (DRBD_DEVICE, DRBD_MOUNTPOINT),
                'res_mysql_drbd':'params drbd_resource="%s"' % DRBD_RESOURCE,
                'res_mysqld':'op monitor interval=5s',
            }

        groups = {
                'grp_mysql':'res_mysql_fs res_mysql_vip res_mysqld',
            }

        ms = {
                'ms_drbd_mysql':'res_mysql_drbd meta notify="true" master-max="1" master-node-max="1" clone-max="2" clone-node-max="1"'
            }

        orders = {
                'ord_drbd_before_mysql':'inf: ms_drbd_mysql:promote grp_mysql:start'
            }

        colocations = {
                'col_mysql_on_drbd':'inf: grp_mysql ms_drbd_mysql:Master'
            }

        utils.relation_set(block_storage=block_storage,
                 block_device=block_device,
                 corosync_bindiface=corosync_bindiface,
                 corosync_mcastport=corosync_mcastport,
                 resources=resources,
                 resource_params=resource_params,
                 init_services=init_services,
                 colocations=colocations,
                 orders=orders,
                 groups=groups,
                 ms=ms)

    elif block_storage == "ceph":
        # If the 'ha' relation has been made *before* the 'ceph' relation,
        # it doesn't make sense to make it until after the 'ceph' relation
        # is made
        if not is_relation_made('ceph'):
            utils.juju_log('INFO',
                           'Relation with ceph does not exists, not sending ha relation data')
            return

        resources = {
                'res_mysql_rbd':'ocf:ceph:rbd',
                'res_mysql_fs':'ocf:heartbeat:Filesystem',
                'res_mysql_vip':'ocf:heartbeat:IPaddr2',
                'res_mysqld':'upstart:mysql',
            }

        resource_params = {
                'res_mysql_rbd':'params name="%s" pool="images" user="%s" secret="%s"' % (
                                config['rbd-name'], SERVICE_NAME, KEYFILE),
                'res_mysql_fs':'params device="/dev/rbd/images/%s" directory="%s" fstype="ext4" op start start-delay="10s"' % (
                                config['rbd-name'], DATA_SRC_DST),
                'res_mysql_vip':'params ip="%s" cidr_netmask="%s" nic="%s"' % (config['vip'],
                                config['vip_cidr'], config['vip_iface']),
                'res_mysqld':'op monitor interval=5s',
            }

        groups = {
                'grp_mysql':'res_mysql_rbd res_mysql_fs res_mysql_vip res_mysqld',
            }

        utils.relation_set(block_storage=block_storage,
                 corosync_bindiface=corosync_bindiface,
                 corosync_mcastport=corosync_mcastport,
                 resources=resources,
                 resource_params=resource_params,
                 init_services=init_services,
                 groups=groups)


def ha_relation_changed():
    # TODO: Add remaining stuff (is_clustered and is_leader)
    return
    relation_data = utils.relation_get_dict()
    if ('clustered' in relation_data and
        utils.is_leader()):
        utils.juju_log('Cluster configured, notifying other services')
        # Tell all related services to start using
        # the VIP
        for r_id in utils.relation_ids('shared-db'):
            utils.relation_set(rid=r_id,
                           db_host=config['vip'])


def ceph_joined():
    ceph_dir = "/etc/ceph"
    if not os.path.isdir(ceph_dir):
        os.mkdir(ceph_dir)
    utils.install('ceph-common')


def ceph_changed():
    # TODO: ask james: What happens if the relation data has changed?
    # do we reconfigure ceph? What do we do with the data?
    key = utils.relation_get('key')

    if key:
        # create KEYRING file
        if not os.path.exists(KEYRING):
            ceph.create_keyring(SERVICE_NAME, KEYRING, key)
        # create a file containing the key
        if not os.path.exists(KEYFILE):
            fd = open(KEYFILE, 'w')
            fd.write(key)
            fd.close()
    else:
        sys.exit(0)

    # emit ceph config
    hosts = get_ceph_nodes()
    mon_hosts = ",".join(map(str, hosts))
    conf_context = {
        'auth': utils.relation_get('auth'),
        'keyring': KEYRING,
        'mon_hosts': mon_hosts,
        }
    with open('/etc/ceph/ceph.conf', 'w') as ceph_conf:
        ceph_conf.write(utils.render_template('ceph.conf',
                                              conf_context))

    # Create the images pool if it does not already exist
    (status, output) = commands.getstatusoutput("rados --id %s lspools | grep images" % SERVICE_NAME)
    if not output:
        utils.juju_log('INFO','Creating image pool')
        ceph.create_image_pool(SERVICE_NAME)

    # Configure ceph()
    configure_ceph()

    # If 'ha' relation has been made before the 'ceph' relation
    # it is important to make sure the ha-relation data is being
    # sent.
    if is_relation_made('ha'):
        utils.juju_log('INFO',
                       'Relation with ha has been done before ceph relation. Making sure the ha relation data is sent')
        ha_relation_joined()
        return


def configure_ceph():
    block_sizemb = int(config['block-size'].split('G')[0]) * 1024
    image_name = config['rbd-name']
    fstype = 'ext4'
    data_src = DATA_SRC_DST
    blk_device = '/dev/rbd/images/%s' % image_name

    # modprobe the kernel module
    utils.juju_log('INFO','Loading kernel module')
    ceph.modprobe_kernel_module('rbd')

    # Stopping MySQL
    if utils.running('mysql'):
        utils.juju_log('INFO','Stopping MySQL before configuring RBD.')
        utils.stop('mysql')

    # configure mysql for ceph storage options
    if utils.get_unit_name() != get_cluster_leader():
        utils.juju_log('INFO','This is not the cluster leader. Not configuring RBD.')
        return

    if utils.get_unit_name() == get_cluster_leader():
        # create an image/block device
        (status, output) = commands.getstatusoutput('rbd list --id %s --pool images' % SERVICE_NAME)
        rbd = image_name in output
        if not rbd:
            utils.juju_log('INFO','Creating RBD Image')
            ceph.create_image(SERVICE_NAME, image_name, str(block_sizemb))
        else:
            utils.juju_log('INFO',
                       'Looks like RBD already exists. Not creating a new one')

        (status, output) = commands.getstatusoutput('rbd showmapped')
        mapped = image_name in output
        if not mapped:
            # map block storage
            utils.juju_log('INFO','Mapping RBD Image as a Block Device')
            ceph.map_block_storage(SERVICE_NAME, image_name, KEYFILE)
        else:
            utils.juju_log('INFO',
                       'Looks like RBD is already mapped. Not re-mapping.')

        # make file system
        # TODO: What happens if for whatever reason this is run again and
        # the data is already in the rbd device and/or is mounted??
        # When it is mounted already, it will fail to make the fs
        utils.juju_log('INFO', 'Trying to move data over to RBD.')
        if not filesystem_mounted(data_src):
            utils.juju_log('INFO', 'Formating RBD.')
            ceph.make_filesystem(SERVICE_NAME, blk_device, fstype)

            utils.juju_log('INFO', 'Copying MySQL data to RBD.')
            # mount block device to temporary location and copy the data
            ceph.place_data_on_ceph(SERVICE_NAME, blk_device, data_src, fstype)

            # Make files be owned by mysql user/pass
            cmd = ['chown', '-R', 'mysql:mysql', data_src]
            subprocess.check_call(cmd)
        else:
            utils.juju_log('INFO',
                       'Looks like data is already on the RBD, skipping...')

        if not utils.running('mysql'):
            utils.start('mysql')


def filesystem_mounted(fs):
    return subprocess.call(['grep', '-wqs', fs, '/proc/mounts']) == 0


def get_ceph_nodes():
    hosts = []
    for r_id in utils.relation_ids('ceph'):
        for unit in utils.relation_list(r_id):
            hosts.append(utils.relation_get_dict(relation_id=r_id,
                                                 remote_unit=unit)['private-address'])
    return hosts


def get_cluster_nodes():
    hosts = []
    hosts.append('{}:6789'.format(utils.get_host_ip()))

    for relid in utils.relation_ids('cluster'):
        for unit in utils.relation_list(relid):
            hosts.append(
                '{}:6789'.format(utils.get_host_ip(
                                    utils.relation_get('private-address',
                                                       unit, relid)))
                )

    hosts.sort()
    return hosts


def get_cluster_leader():
    # Obtains the unit name of the first service unit deploy.
    # e.g. mysql-0
    units = []
    local = units.append(utils.get_unit_name())
    for r_id in utils.relation_ids('cluster'):
        for unit in utils.relation_list(r_id):
            units.append(unit.replace('/','-'))

    return min(unit for unit in units)


def get_drbd_conf():
    cluster_hosts = {}
    # TODO: In MAAS private-address is the *hostname*. We need to set the
    # private address in a relation.
    cluster_hosts[utils.get_unit_hostname()] = utils.unit_get('private-address')
    for r_id in utils.relation_ids('cluster'):
        for unit in utils.relation_list(r_id):
            cluster_hosts[unit.replace('/','-')] = \
                utils.relation_get_dict(relation_id=r_id,
                                  remote_unit=unit)['private-address']

    conf = {
            'block_device': config['block-device'],
            'drbd_device': DRBD_DEVICE,
            'units': cluster_hosts,
                }
    return conf


def emit_drbd_conf():
    # read config variables
    drbd_conf_context = get_drbd_conf()
    # write config file
    with open('/etc/drbd.d/mysql.res', 'w') as drbd_conf:
        drbd_conf.write(utils.render_template('mysql.res',
                                              drbd_conf_context))


def configure_drbd():
    # Check that we are not already configured
    if os.path.exists(STORAGEMARKER):
        utils.juju_log('INFO',
                       'Block storage already configured, not reconfiguring')
        return

    # Check for a defined block device.
    if config['block-device'] == "None":
        utils.juju_log('WARNING',
                       'NO block-device defined, cannot configure DRBD')
        return

    drbd.prepare_drbd_disk(config['block-device'])
    drbd.modprobe_module()

    emit_drbd_conf()
    drbd.create_md(DRBD_RESOURCE)
    drbd.bring_resource_up(DRBD_RESOURCE)

    # Wait for quorum.
    while not drbd.is_quorum_secondary():
        time.sleep(1)
    while not drbd.is_state_inconsistent():
        time.sleep(1)

    if utils.get_unit_name() == get_cluster_leader():
        # clear bitmap
        if drbd.is_quorum_secondary() and drbd.is_state_inconsistent():
            drbd.clear_bitmap(DRBD_RESOURCE)
        # wait for resources to be UpToDate
        while not drbd.is_state_uptodate():
            time.sleep(1)
        # Make leader primary
        if drbd.is_state_uptodate():
            drbd.make_primary(DRBD_RESOURCE)
        # Wait for node to become primary
        while not drbd.is_quorum_primary():
           time.sleep(1)
        # Format DRBD resource
        if drbd.is_quorum_primary():
            drbd.format_drbd_device()

    utils.stop("mysql")
    if utils.get_unit_name() == get_cluster_leader() and drbd.is_quorum_primary():
       drbd.put_on_drbd()
       utils.start("mysql")

    # TODO: if leader fails, then marker should not be placed on secondary node, nor
    # DRBD should be mounted on both.
    # TODO: probably would be good idea to check that DRBD has been mounted in var/lib/mysql
    # in primary and secondary, that should say it has been successful.
    with open(STORAGEMARKER, 'w') as marker:
        marker.write('done')


def cluster_changed():
    utils.juju_log('INFO', 'Begin cluster changed hook.')

    if config['block-storage'] == "None":
        utils.juju_log('WARNING', 'NO block storage configured, bailing')
        return
    if config['block-size'] == "None":
        utils.juju_log('WARNING', 'NO block storage size configured, bailing')
        return

    if config['block-storage'] == "drbd":
        if len(get_cluster_nodes()) != 2:
            utils.juju_log('WARNING', 'Not enough nodes in cluster, bailing')
            return
        else:
            configure_drbd()
    elif config['block-storage'] == "ceph":
        pass

    utils.juju_log('INFO', 'End install hook.')


hooks = {
    "cluster-relation-changed": cluster_changed,
    "ha-relation-joined": ha_relation_joined,
    "ha-relation-changed": ha_relation_changed,
    "ceph-relation-joined": ceph_joined,
    "ceph-relation-changed": ceph_changed,
}

# keystone-hooks gets called by symlink corresponding to the requested relation
# hook.
arg0 = sys.argv[0].split("/").pop()
if arg0 not in hooks.keys():
    error_out("Unsupported hook: %s" % arg0)
hooks[arg0]()
